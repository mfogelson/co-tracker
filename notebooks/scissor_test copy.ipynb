{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a7e08e-93c6-4370-9778-3bb102dce78b",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081cd8f-f6f9-4a1a-8c36-8a857b0c3b03",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3240f-0354-4802-b8b5-9070930fc957",
   "metadata": {},
   "source": [
    "# CoTracker: It is Better to Track Together\n",
    "This is a demo for <a href=\"https://co-tracker.github.io/\">CoTracker</a>, a model that can track any point in a video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff1fd0-572e-47fb-8221-1e73ac17cfd1",
   "metadata": {},
   "source": [
    "<img src=\"https://www.robots.ox.ac.uk/~nikita/storage/cotracker/bmx-bumps.gif\" alt=\"Logo\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6db31",
   "metadata": {},
   "source": [
    "Don't forget to turn on GPU support if you're running this demo in Colab. \n",
    "\n",
    "**Runtime** -> **Change runtime type** -> **Hardware accelerator** -> **GPU**\n",
    "\n",
    "Let's install dependencies for Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1745a859-71d4-4ec3-8ef3-027cabe786d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from base64 import b64encode\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894bd2d-2099-46fa-8286-f0c56298ecd1",
   "metadata": {},
   "source": [
    "Read a video from CO3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f9ca4d-951e-49d2-8844-91f7bcadfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = '../paper_data/video_6/'\n",
    "video_path = os.path.join(video_dir, 'orig/video_6_rotated.mov')\n",
    "video = read_video_from_path(video_path)\n",
    "video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "df4fde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def rotate_and_save_video(input_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (height, width))  # Note the swapped height and width\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Rotate the frame 90 degrees clockwise\n",
    "            rotated_frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "            \n",
    "            # Write the rotated frame\n",
    "            out.write(rotated_frame)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Your existing code\n",
    "video_dir = '../paper_data/video_6/'\n",
    "input_video_path = os.path.join(video_dir, 'orig/video_6.mp4')\n",
    "output_video_path = os.path.join(video_dir, 'orig/video_6_rotated.mp4')\n",
    "\n",
    "# Rotate and save the video\n",
    "rotate_and_save_video(input_video_path, output_video_path)\n",
    "\n",
    "# If you still need the video as a torch tensor\n",
    "video = cv2.VideoCapture(output_video_path)\n",
    "frames = []\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    else:\n",
    "        break\n",
    "video.release()\n",
    "\n",
    "video = np.stack(frames)\n",
    "video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c2e9d-0e85-4c10-81a2-827d0759bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(video_path):\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width=\"640\" height=\"480\" autoplay loop controls><source src=\"{video_url}\"></video>\"\"\")\n",
    " \n",
    "show_video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89ae18-54d0-4384-8a79-ca9247f5f31a",
   "metadata": {},
   "source": [
    "Import CoTrackerPredictor and create an instance of it. We'll use this object to estimate tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ac40b-bde8-46d4-bd57-4ead939f22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotracker.predictor import CoTrackerPredictor\n",
    "\n",
    "model = CoTrackerPredictor(\n",
    "    checkpoint=os.path.join(\n",
    "        '../checkpoints/cotracker2v1.pth'\n",
    "    ), \n",
    "    window_len=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2a4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    video = video.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d88a5f-057c-4b9f-828d-ee0b97d1e72f",
   "metadata": {},
   "source": [
    "## Tracking manually selected points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bca85-b872-4f4e-be19-ff16f0984037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We will start by tracking points queried manually.\n",
    "We define a queried point as: [time, x coord, y coord] \n",
    "\n",
    "So, the code below defines points with different x and y coordinates sampled on frames 0, 10, 20, and 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2730a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Callback function for trackbars (required by OpenCV, but does nothing here)\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# Load a sample image or video frame to find the color bounds\n",
    "image_path = '/Users/mitchfogelson/Library/CloudStorage/Box-Box/00_Mitch Fogelson/00_Research/00_Niac_Space_Structures/09_Closed_Loop_Simulation/Paper_data/meshcat_1727874311177/0000000.png'  # Replace with your image or capture from the video\n",
    "image = cv2.imread(image_path)\n",
    "# image = cv2.resize(image, (600, 400))  # Resize for easier display\n",
    "\n",
    "# Create a window\n",
    "cv2.namedWindow('Trackbars')\n",
    "\n",
    "# Create trackbars for adjusting HSV values\n",
    "cv2.createTrackbar('H Lower', 'Trackbars', 0, 180, nothing)\n",
    "cv2.createTrackbar('H Upper', 'Trackbars', 180, 180, nothing)\n",
    "cv2.createTrackbar('S Lower', 'Trackbars', 0, 255, nothing)\n",
    "cv2.createTrackbar('S Upper', 'Trackbars', 255, 255, nothing)\n",
    "cv2.createTrackbar('V Lower', 'Trackbars', 0, 255, nothing)\n",
    "cv2.createTrackbar('V Upper', 'Trackbars', 255, 255, nothing)\n",
    "\n",
    "while True:\n",
    "    # Convert image to HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Get current positions of all trackbars\n",
    "    h_lower = cv2.getTrackbarPos('H Lower', 'Trackbars')\n",
    "    h_upper = cv2.getTrackbarPos('H Upper', 'Trackbars')\n",
    "    s_lower = cv2.getTrackbarPos('S Lower', 'Trackbars')\n",
    "    s_upper = cv2.getTrackbarPos('S Upper', 'Trackbars')\n",
    "    v_lower = cv2.getTrackbarPos('V Lower', 'Trackbars')\n",
    "    v_upper = cv2.getTrackbarPos('V Upper', 'Trackbars')\n",
    "\n",
    "    # Set lower and upper bounds for HSV values\n",
    "    lower_bound = np.array([h_lower, s_lower, v_lower])\n",
    "    upper_bound = np.array([h_upper, s_upper, v_upper])\n",
    "\n",
    "    # Create a mask based on the HSV range set by the trackbars\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Display the original image and the masked result\n",
    "    cv2.imshow('Original', image)\n",
    "    cv2.imshow('Mask', mask)\n",
    "    cv2.imshow('Result', result)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def select_roi(frame):\n",
    "    \"\"\"Allow user to select a region of interest (ROI) on the frame.\"\"\"\n",
    "    roi = cv2.selectROI(\"Select Region of Interest\", frame, fromCenter=False, showCrosshair=True)\n",
    "    cv2.destroyWindow(\"Select Region of Interest\")\n",
    "    return roi\n",
    "\n",
    "def detect_points(frame, roi, blue_lower, blue_upper):\n",
    "    \"\"\"Detect blue points in the frame within the ROI.\"\"\"\n",
    "    roi_frame = frame[int(roi[1]):int(roi[1]+roi[3]), int(roi[0]):int(roi[0]+roi[2])]\n",
    "    hsv = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2HSV)\n",
    "    blue_mask = cv2.inRange(hsv, blue_lower, blue_upper)\n",
    "    contours, _ = cv2.findContours(blue_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True)[:28]#[:68]\n",
    "    \n",
    "    points = []\n",
    "    for contour in contours:\n",
    "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "        x, y = int(x) + int(roi[0]), int(y) + int(roi[1])\n",
    "        points.append((x, y))\n",
    "    \n",
    "    return points\n",
    "\n",
    "def correct_points(frame, points):\n",
    "    \"\"\"Allow user to correct point positions by dragging.\"\"\"\n",
    "    corrected_points = points.copy()\n",
    "    window_name = \"Correct Points (Drag to move, press Enter when done, 'r' to reset a point)\"\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    dragging = False\n",
    "    drag_point_index = -1\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal dragging, drag_point_index, corrected_points\n",
    "        \n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            drag_point_index = min(range(len(corrected_points)), \n",
    "                                   key=lambda i: (corrected_points[i][0]-x)**2 + (corrected_points[i][1]-y)**2)\n",
    "            dragging = True\n",
    "        \n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if dragging:\n",
    "                corrected_points[drag_point_index] = (x, y)\n",
    "        \n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            dragging = False\n",
    "            drag_point_index = -1\n",
    "        \n",
    "        frame_copy = frame.copy()\n",
    "        for i, (px, py) in enumerate(corrected_points):\n",
    "            cv2.circle(frame_copy, (px, py), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(frame_copy, f'ID:{i}', (px, py - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.imshow(window_name, frame_copy)\n",
    "    \n",
    "    cv2.setMouseCallback(window_name, mouse_callback)\n",
    "    \n",
    "    while True:\n",
    "        frame_copy = frame.copy()\n",
    "        for i, (x, y) in enumerate(corrected_points):\n",
    "            cv2.circle(frame_copy, (x, y), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(frame_copy, f'ID:{i}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.imshow(window_name, frame_copy)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 13:  # Press Enter to finish correction\n",
    "            break\n",
    "        elif key == ord('r'):  # Press 'r' to reset a point\n",
    "            print(\"Click on the point you want to reset.\")\n",
    "            reset_point = cv2.waitKey(0) & 0xFF\n",
    "            if reset_point >= ord('0') and reset_point <= ord('9'):\n",
    "                index = reset_point - ord('0')\n",
    "                if index < len(corrected_points):\n",
    "                    corrected_points[index] = points[index]  # Reset to original position\n",
    "    \n",
    "    cv2.destroyWindow(window_name)\n",
    "    return corrected_points\n",
    "\n",
    "# Load the video\n",
    "# video_path = \"/Users/mitchfogelson/Downloads/trimmed_scissor_video.mov\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read the first frame\n",
    "ret, first_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the video\")\n",
    "    exit()\n",
    "\n",
    "# first_frame = image #cv2.resize(image, (600, 400))  # Resize for easier display\n",
    "# Let the user select the ROI\n",
    "roi = select_roi(first_frame)\n",
    "\n",
    "# Define HSV color range for blue dots\n",
    "# blue_lower = np.array([100, 20, 14])\n",
    "blue_lower = np.array([100, 56, 0])\n",
    "blue_upper = np.array([180, 255, 255])\n",
    "\n",
    "# Detect initial points\n",
    "initial_points = detect_points(first_frame, roi, blue_lower, blue_upper)\n",
    "\n",
    "# Allow user to correct initial points\n",
    "corrected_points = correct_points(first_frame, initial_points)\n",
    "\n",
    "# Create the output vector\n",
    "output_vector = [[0, x, y] for x, y in corrected_points]\n",
    "\n",
    "# Print the output vector\n",
    "print(\"Output vector [frame #, xpos, ypos]:\")\n",
    "for point in output_vector:\n",
    "    print(point)\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6422e7c-8c6f-4269-92c3-245344afe35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.tensor(output_vector).float()[None].reshape(-1, 3)\n",
    "if torch.cuda.is_available():\n",
    "    queries = queries.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2727bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7693b-9d74-48b3-b612-360290ff1e7a",
   "metadata": {},
   "source": [
    "We pass these points as input to the model and track them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09008ca9-6a87-494f-8b05-6370cae6a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tracks, pred_visibility = model(video, queries=queries[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d2a35-3daf-482d-b40b-b6d4f548ca40",
   "metadata": {},
   "source": [
    "Finally, we visualize the results with tracks leaving traces from the frame where the tracking starts.\n",
    "Color encodes time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01467f8d-667c-4f41-b418-93132584c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Visualizer(\n",
    "    save_dir=os.path.join(video_dir, \"tracking\"),\n",
    "    linewidth=6,\n",
    "    mode='cool',\n",
    "    tracks_leave_trace=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis.visualize(\n",
    "    video=video,\n",
    "    tracks=pred_tracks,\n",
    "    visibility=pred_visibility,\n",
    "    filename='queries');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23d210-ed90-49f1-8311-b7e354c7a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(os.path.join(video_dir, \"tracking/queries.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ecb50-f707-406a-bc64-94bfded827a8",
   "metadata": {},
   "source": [
    "Notice that points queried at frames 10, 20, and 30 are tracked **incorrectly** before the query frame. This is because CoTracker is an online algorithm and only tracks points in one direction. However, we can also run it backward from the queried point to track in both directions. Let's correct this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def select_roi(frame):\n",
    "    \"\"\"Allow user to select a region of interest (ROI) on the frame.\"\"\"\n",
    "    roi = cv2.selectROI(\"Select Region of Interest\", frame, fromCenter=False, showCrosshair=True)\n",
    "    cv2.destroyWindow(\"Select Region of Interest\")\n",
    "    return roi\n",
    "\n",
    "def detect_points(frame, roi, blue_lower, blue_upper):\n",
    "    \"\"\"Detect blue points in the frame within the ROI.\"\"\"\n",
    "    roi_frame = frame[int(roi[1]):int(roi[1]+roi[3]), int(roi[0]):int(roi[0]+roi[2])]\n",
    "    hsv = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2HSV)\n",
    "    blue_mask = cv2.inRange(hsv, blue_lower, blue_upper)\n",
    "    contours, _ = cv2.findContours(blue_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True)[:28] #[:68]\n",
    "    \n",
    "    points = []\n",
    "    for contour in contours:\n",
    "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "        x, y = int(x) + int(roi[0]), int(y) + int(roi[1])\n",
    "        points.append((x, y))\n",
    "    \n",
    "    return points\n",
    "\n",
    "def correct_points(frame, points):\n",
    "    \"\"\"Allow user to correct point positions by dragging.\"\"\"\n",
    "    corrected_points = points.copy()\n",
    "    window_name = \"Correct Points (Drag to move, press Enter when done, 'r' to reset a point)\"\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    dragging = False\n",
    "    drag_point_index = -1\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal dragging, drag_point_index, corrected_points\n",
    "        \n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            drag_point_index = min(range(len(corrected_points)), \n",
    "                                   key=lambda i: (corrected_points[i][0]-x)**2 + (corrected_points[i][1]-y)**2)\n",
    "            dragging = True\n",
    "        \n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if dragging:\n",
    "                corrected_points[drag_point_index] = (x, y)\n",
    "        \n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            dragging = False\n",
    "            drag_point_index = -1\n",
    "        \n",
    "        frame_copy = frame.copy()\n",
    "        for i, (px, py) in enumerate(corrected_points):\n",
    "            cv2.circle(frame_copy, (px, py), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(frame_copy, f'ID:{i}', (px, py - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.imshow(window_name, frame_copy)\n",
    "    \n",
    "    cv2.setMouseCallback(window_name, mouse_callback)\n",
    "    \n",
    "    while True:\n",
    "        frame_copy = frame.copy()\n",
    "        for i, (x, y) in enumerate(corrected_points):\n",
    "            cv2.circle(frame_copy, (x, y), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(frame_copy, f'ID:{i}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.imshow(window_name, frame_copy)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 13:  # Press Enter to finish correction\n",
    "            break\n",
    "        elif key == ord('r'):  # Press 'r' to reset a point\n",
    "            print(\"Click on the point you want to reset.\")\n",
    "            reset_point = cv2.waitKey(0) & 0xFF\n",
    "            if reset_point >= ord('0') and reset_point <= ord('9'):\n",
    "                index = reset_point - ord('0')\n",
    "                if index < len(corrected_points):\n",
    "                    corrected_points[index] = points[index]  # Reset to original position\n",
    "    \n",
    "    cv2.destroyWindow(window_name)\n",
    "    return corrected_points\n",
    "\n",
    "# Load the video\n",
    "# video_path = \"/Users/mitchfogelson/Downloads/trimmed_scissor_video.mov\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize last_frame\n",
    "last_frame = None\n",
    "frame_count = 0\n",
    "\n",
    "# Read frames until the end of the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    last_frame = frame\n",
    "    frame_count += 1\n",
    "\n",
    "if last_frame is None:\n",
    "    print(\"Failed to read any frames from the video\")\n",
    "    # exit()\n",
    "\n",
    "print(f\"Total frames in video: {frame_count}\")\n",
    "\n",
    "# Let the user select the ROI\n",
    "roi = select_roi(last_frame)\n",
    "\n",
    "# Define HSV color range for blue dots\n",
    "# blue_lower = np.array([100, 20, 14])\n",
    "# blue_upper = np.array([180, 255, 255])\n",
    "blue_lower = np.array([100, 78, 54])\n",
    "blue_upper = np.array([180, 255, 255])\n",
    "\n",
    "# Detect initial points\n",
    "initial_points = detect_points(last_frame, roi, blue_lower, blue_upper)\n",
    "\n",
    "# Allow user to correct initial points\n",
    "corrected_points = correct_points(last_frame, initial_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a98ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the output vector\n",
    "output_vector = [[frame_count-1, x, y] for x, y in corrected_points]\n",
    "\n",
    "# Print the output vector\n",
    "print(\"Output vector [frame #, xpos, ypos]:\")\n",
    "for point in output_vector:\n",
    "    print(point)\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e32fcf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.tensor(output_vector).float()[None].reshape(-1, 3)\n",
    "if torch.cuda.is_available():\n",
    "    queries = queries.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b40775f2-6ab0-4bc6-9099-f903935657c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tracks, pred_visibility = model(video, queries=queries[None], backward_tracking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de508af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tracks[0,0,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis.visualize(\n",
    "    video=video,\n",
    "    tracks=pred_tracks,\n",
    "    visibility=pred_visibility,\n",
    "    filename='queries_backward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3120f31-9365-4867-8c85-5638b0708edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(os.path.join(video_dir, \"tracking/queries_backward.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def select_roi(frame):\n",
    "    \"\"\"Allow user to select a region of interest (ROI) on the frame.\"\"\"\n",
    "    roi = cv2.selectROI(\"Select Region of Interest\", frame, fromCenter=False, showCrosshair=True)\n",
    "    cv2.destroyWindow(\"Select Region of Interest\")\n",
    "    return roi\n",
    "\n",
    "def select_scale_points(frame):\n",
    "    \"\"\"Allow user to select two points for setting the physical scale.\"\"\"\n",
    "    points = []\n",
    "    \n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            points.append((x, y))\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "            cv2.imshow(\"Select Scale Points\", frame)\n",
    "            \n",
    "            if len(points) == 2:\n",
    "                cv2.line(frame, points[0], points[1], (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Select Scale Points\", frame)\n",
    "    \n",
    "    cv2.namedWindow(\"Select Scale Points\")\n",
    "    cv2.setMouseCallback(\"Select Scale Points\", mouse_callback)\n",
    "    \n",
    "    while len(points) < 2:\n",
    "        cv2.imshow(\"Select Scale Points\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyWindow(\"Select Scale Points\")\n",
    "    return points\n",
    "\n",
    "def calculate_scale(points, distance_meters):\n",
    "    \"\"\"Calculate the scale (pixels per meter) based on two points and their real-world distance.\"\"\"\n",
    "    pixel_distance = np.sqrt((points[1][0] - points[0][0])**2 + (points[1][1] - points[0][1])**2)\n",
    "    return pixel_distance / distance_meters\n",
    "\n",
    "def pixels_to_meters(points, scale, origin):\n",
    "    \"\"\"Convert pixel coordinates to meters.\"\"\"\n",
    "    return [((p[0] - origin[0]) / scale, (p[1] - origin[1]) / scale) for p in points]\n",
    "\n",
    "# Load the video\n",
    "# video_path = '/Users/mitchfogelson/Downloads/IMG_2071 2.mov'  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read the first frame (or last frame, depending on your preference)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the video\")\n",
    "    exit()\n",
    "\n",
    "# Let the user select the ROI\n",
    "roi = select_roi(frame)\n",
    "\n",
    "# Let the user select two points for scale\n",
    "scale_points = select_scale_points(frame)\n",
    "\n",
    "# Ask for the real-world distance between the two points\n",
    "distance_meters = float(input(\"Enter the distance between the two points in meters: \"))\n",
    "\n",
    "# Calculate the scale (pixels per meter)\n",
    "scale = calculate_scale(scale_points, distance_meters)\n",
    "print(f\"Scale: {scale} pixels per meter\")\n",
    "\n",
    "# Set the origin (top-left corner of ROI)\n",
    "origin = (int(roi[0]), int(roi[1]))\n",
    "\n",
    "# Now, let's process the pred_tracks\n",
    "# Assuming pred_tracks is already loaded and has shape [1, # frames, # nodes, 2]\n",
    "# pred_tracks = np.random.rand(1, 100, 10, 2) * 100  # Replace this with your actual pred_tracks\n",
    "\n",
    "# Convert pred_tracks from pixels to meters\n",
    "pred_tracks_meters = np.zeros_like(pred_tracks)\n",
    "for i in range(pred_tracks.shape[1]):  # For each frame\n",
    "    for j in range(pred_tracks.shape[2]):  # For each node\n",
    "        x_pixels, y_pixels = pred_tracks[0, i, j]\n",
    "        x_meters, y_meters = pixels_to_meters([(x_pixels, y_pixels)], scale, origin)[0]\n",
    "        pred_tracks_meters[0, i, j] = [x_meters, y_meters]\n",
    "\n",
    "# Print some sample results\n",
    "print(\"\\nSample results (first 5 frames, first 3 nodes):\")\n",
    "print(\"Frame | Node | Pixels (x, y) | Meters (x, y)\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(min(5, pred_tracks.shape[1])):\n",
    "    for j in range(min(3, pred_tracks.shape[2])):\n",
    "        pixels = pred_tracks[0, i, j]\n",
    "        meters = pred_tracks_meters[0, i, j]\n",
    "        print(f\"{i:5d} | {j:4d} | ({pixels[0]:6.2f}, {pixels[1]:6.2f}) | ({meters[0]:6.2f}, {meters[1]:6.2f})\")\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from moviepy.editor import VideoFileClip\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def create_animation(pred_tracks_meters, output_file='animation.mp4', fps=30, dot_size=5):\n",
    "    \"\"\"\n",
    "    Create an animation of the points from pred_tracks_meters.\n",
    "    \n",
    "    Args:\n",
    "    pred_tracks_meters (np.array): Array of shape [1, # frames, # nodes, 2] containing the x, y coordinates in meters.\n",
    "    output_file (str): Name of the output video file.\n",
    "    fps (int): Frames per second for the output video.\n",
    "    dot_size (int): Size of the dots representing the points.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract relevant dimensions\n",
    "    num_frames = pred_tracks_meters.shape[1]\n",
    "    num_nodes = pred_tracks_meters.shape[2]\n",
    "    \n",
    "    # Create a new figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    #square aspect ratio\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Set the limits of the plot\n",
    "    x_min, x_max = pred_tracks_meters[0, :, :, 0].min(), pred_tracks_meters[0, :, :, 0].max()\n",
    "    y_min, y_max = pred_tracks_meters[0, :, :, 1].min(), pred_tracks_meters[0, :, :, 1].max()\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Create a scatter plot for the points\n",
    "    scatter = ax.scatter([], [], s=dot_size)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X position (meters)')\n",
    "    ax.set_ylabel('Y position (meters)')\n",
    "    ax.set_title('Point Tracking Animation')\n",
    "    \n",
    "    # Add a text annotation for the frame number\n",
    "    frame_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, verticalalignment='top')\n",
    "    \n",
    "    def init():\n",
    "        scatter.set_offsets(np.c_[[], []])\n",
    "        frame_text.set_text('')\n",
    "        return scatter, frame_text\n",
    "    \n",
    "    def update(frame):\n",
    "        # Update the positions of the points\n",
    "        positions = pred_tracks_meters[0, frame, :, :]\n",
    "        scatter.set_offsets(positions)\n",
    "        \n",
    "        # Update the frame number text\n",
    "        frame_text.set_text(f'Frame: {frame}')\n",
    "        \n",
    "        return scatter, frame_text\n",
    "    \n",
    "    # Create the animation\n",
    "    anim = FuncAnimation(fig, update, frames=num_frames, init_func=init, blit=True, interval=1000/fps)\n",
    "    \n",
    "    # Save the animation as a video file\n",
    "    writer = animation.writers['ffmpeg'](fps=fps)\n",
    "    anim.save(output_file, writer=writer)\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Animation saved as {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming pred_tracks_meters is already defined\n",
    "create_animation(pred_tracks_meters, output_file=os.path.join(video_dir, 'tracking/point_tracking_animation.mp4'), fps=30, dot_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d451b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_pred_tracks_to_csv(pred_tracks_meters, filename):\n",
    "    \"\"\"\n",
    "    Save pred_tracks_meters to a CSV file with the following format:\n",
    "    Headers: node ids\n",
    "    Rows: (x,y) for each frame\n",
    "\n",
    "    Args:\n",
    "    pred_tracks_meters (np.array): Array of shape [1, # frames, # nodes, 2] containing the x, y coordinates in meters.\n",
    "    filename (str): Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Remove the first dimension if it's 1\n",
    "    if pred_tracks_meters.shape[0] == 1:\n",
    "        pred_tracks_meters = pred_tracks_meters.squeeze(0)\n",
    "\n",
    "    num_frames, num_nodes, _ = pred_tracks_meters.shape\n",
    "\n",
    "    # Create headers (node IDs)\n",
    "    headers = [f'Node_{i}' for i in range(num_nodes)]\n",
    "\n",
    "    # Create a list to store each row of data\n",
    "    data_rows = []\n",
    "\n",
    "    # Process each frame\n",
    "    for frame in range(num_frames):\n",
    "        row = []\n",
    "        for node in range(num_nodes):\n",
    "            x, y = pred_tracks_meters[frame, node]\n",
    "            row.append(f'({x:.4f},{y:.4f})')\n",
    "        data_rows.append(row)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "    # Add a frame number column\n",
    "    df.insert(0, 'Frame', range(num_frames))\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming pred_tracks_meters is already defined\n",
    "if not os.path.exists(os.path.join(video_dir, 'csv')):\n",
    "    os.makedirs(os.path.join(video_dir, 'csv'))\n",
    "save_pred_tracks_to_csv(pred_tracks_meters, os.path.join(video_dir, 'csv/pred_tracks_formatted.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2a3b4-a8b3-4aeb-87d2-28f056c624ba",
   "metadata": {},
   "source": [
    "## Get slop from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a86601da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "571fb7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(video_dir, 'csv/pred_tracks_formatted.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7227ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "237f4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = np.asarray([[float(df.iloc[frame,1:][i].replace('(','').replace(')','').split(',')[0]), float(df.iloc[frame,1:][i].replace('(','').replace(')','').split(',')[1])] for i in range(df.shape[1]-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e066e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the data \n",
    "plt.figure(figsize=(10, 10))\n",
    "# for i in range(1, df.shape[1]):\n",
    "plt.scatter(positions[:,0], positions[:,1],  s=10)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Position (meters)')\n",
    "plt.title('Point Tracking Data')\n",
    "# plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0204825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 3 points with the smallest x values\n",
    "sorted_positions = positions[positions[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "643e5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_pos = sorted_positions[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56cf8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_points = [p for p in sorted_positions if np.linalg.norm(p[1]-center_pos) <= 1.055e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d46340d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_points(sorted_positions, tolerance=5e-3):\n",
    "    grouped_points = []\n",
    "    \n",
    "    center = sorted_positions[0,1]\n",
    "\n",
    "    # Get the center points that are close to the scissor center\n",
    "    center_points = [p for p in sorted_positions if np.linalg.norm(p[1] - center) <= tolerance]\n",
    "    center_points_ind = [i for i, p in enumerate(sorted_positions) if np.linalg.norm(p[1] - center) <= tolerance]\n",
    "    \n",
    "    # get all the points that are not the center points \n",
    "    sorted_positions_reduced = np.delete(sorted_positions, center_points_ind, axis=0)\n",
    "    \n",
    "    for center_point in center_points:\n",
    "        # Find the 4 points in the [top right, bottom left, bottom right, top left] quadrants\n",
    "        top_right = min([p for p in sorted_positions_reduced if p[0] > center_point[0] and p[1] > center_point[1]],\n",
    "                        key=lambda p: np.linalg.norm(p - center_point), default=None)\n",
    "        bottom_left = min([p for p in sorted_positions_reduced if p[0] < center_point[0] and p[1] < center_point[1]],\n",
    "                            key=lambda p: np.linalg.norm(p - center_point), default=None)\n",
    "        bottom_right = min([p for p in sorted_positions_reduced if p[0] > center_point[0] and p[1] < center_point[1]],\n",
    "                            key=lambda p: np.linalg.norm(p - center_point), default=None)\n",
    "        top_left = min([p for p in sorted_positions_reduced if p[0] < center_point[0] and p[1] > center_point[1]],\n",
    "                        key=lambda p: np.linalg.norm(p - center_point), default=None)\n",
    "\n",
    "        grouped_points.append({\n",
    "            'center': center_point,\n",
    "            'points': [top_right, bottom_left, bottom_right, top_left]\n",
    "        })\n",
    "\n",
    "    return grouped_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6531fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(center_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4515f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_points_real = group_points(sorted_positions, 1.055e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_points_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(center_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9719d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_positions[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "221a9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the angle between the 3 points\n",
    "def angle_between_points(p1, p2, p3):\n",
    "    \"\"\"Calculate the angle between three points.\"\"\"\n",
    "    v1 = p1 - p2\n",
    "    v2 = p3 - p2\n",
    "    angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "    return np.degrees(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad1b342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = angle_between_points(sorted_positions[0], sorted_positions[1], sorted_positions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3f2351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_2_points(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate the angle (in degrees) between the line formed by two points (p1, p2) and the x-axis.\n",
    "    \n",
    "    Parameters:\n",
    "    p1 (tuple): Coordinates of the first point (x1, y1)\n",
    "    p2 (tuple): Coordinates of the second point (x2, y2)\n",
    "    \n",
    "    Returns:\n",
    "    float: Angle between the line through the points and the x-axis, in degrees.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in x and y coordinates\n",
    "    delta_x = p2[0] - p1[0]\n",
    "    delta_y = p2[1] - p1[1]\n",
    "    \n",
    "    # Calculate the angle using atan2, then convert it to degrees\n",
    "    angle_rad = np.arctan2(delta_y, delta_x)\n",
    "    angle_deg = np.rad2deg(angle_rad)\n",
    "    \n",
    "    return angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b9cd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_angle = angle_between_2_points(sorted_positions[0], sorted_positions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebfd9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.linalg.norm(sorted_positions[0] - sorted_positions[1])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de46ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScissorMechanism:\n",
    "    def __init__(self, link_length, angle, offset_position, offset_rotation, num_cells):\n",
    "        self.link_length = link_length\n",
    "        self.angle = np.deg2rad(angle)  # Convert angle to radians\n",
    "        self.offset_position = np.array(offset_position)\n",
    "        self.offset_rotation = np.deg2rad(offset_rotation)  # Convert offset rotation to radians\n",
    "        self.num_cells = num_cells\n",
    "\n",
    "    def calculate_endpoints(self):\n",
    "        \"\"\"\n",
    "        Calculate the positions of the center and end points for each scissor unit.\n",
    "        Returns a list of center and endpoint positions.\n",
    "        \"\"\"\n",
    "        # List to hold the position of the centers and endpoints of all scissor units\n",
    "        positions = []\n",
    "\n",
    "        # Loop over the number of scissor cells\n",
    "        for i in range(self.num_cells):\n",
    "            # Calculate the x and y positions based on the link length and angle for each cell\n",
    "            center_x = self.offset_position[0] + i * self.link_length * np.cos(self.angle/2)\n",
    "            center_y = self.offset_position[1]\n",
    "            center_position = np.array([center_x, center_y])\n",
    "\n",
    "            # Endpoints of the scissor link (left and right)\n",
    "            endpoint_1_x = center_x + 0.5 * self.link_length * np.cos(self.angle/2)\n",
    "            endpoint_1_y = center_y + 0.5 * self.link_length * np.sin(self.angle/2)\n",
    "            endpoint_2_x = center_x - 0.5 * self.link_length * np.cos(self.angle/2)\n",
    "            endpoint_2_y = center_y - 0.5 * self.link_length * np.sin(self.angle/2)\n",
    "            # Endpoints of the scissor link (left and right)\n",
    "            endpoint_3_x = center_x + 0.5 * self.link_length * np.cos(-self.angle/2)\n",
    "            endpoint_3_y = center_y + 0.5 * self.link_length * np.sin(-self.angle/2)\n",
    "            endpoint_4_x = center_x - 0.5 * self.link_length * np.cos(-self.angle/2)\n",
    "            endpoint_4_y = center_y - 0.5 * self.link_length * np.sin(-self.angle/2)\n",
    "\n",
    "\n",
    "            endpoint_1 = np.array([endpoint_1_x, endpoint_1_y])\n",
    "            endpoint_2 = np.array([endpoint_2_x, endpoint_2_y])\n",
    "            endpoint_3 = np.array([endpoint_3_x, endpoint_3_y])\n",
    "            endpoint_4 = np.array([endpoint_4_x, endpoint_4_y])\n",
    "\n",
    "            # Append positions of center and endpoints for this scissor unit\n",
    "            positions.append({\n",
    "                'center': center_position,\n",
    "                'endpoints': (endpoint_1, endpoint_2, endpoint_3, endpoint_4)\n",
    "            })\n",
    "\n",
    "        return positions\n",
    "\n",
    "    def display_positions(self):\n",
    "        \"\"\"\n",
    "        Display the center and endpoints for each scissor unit.\n",
    "        \"\"\"\n",
    "        positions = self.calculate_endpoints()\n",
    "        for i, pos in enumerate(positions):\n",
    "            print(f\"Scissor Unit {i+1}:\")\n",
    "            print(f\"  Center: {pos['center']}\")\n",
    "            print(f\"  Endpoints: {pos['endpoints']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e624d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the scissor mechanism\n",
    "link_length = length # Length of each scissor link\n",
    "angle = 180-angle  # Angle between scissor arms\n",
    "offset_position = sorted_positions[0,:]  # Starting position of the first scissor center\n",
    "offset_rotation = offset_angle  # No rotation applied to the entire scissor mechanism\n",
    "num_cells = 10  # Number of scissor units\n",
    "\n",
    "# Create the scissor mechanism and display positions\n",
    "scissor = ScissorMechanism(link_length, angle, offset_position, offset_rotation, num_cells)\n",
    "scissor.display_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68957085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_v_sim(scissor_positions, grouped_points_real, frame_id, show=False, save=False, video_dir=video_dir):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # for i in range(1, df.shape[1]):\n",
    "    # scissor_positions = scissor.calculate_endpoints()\n",
    "    # Plot the scissor mechanism\n",
    "    for unit in scissor_positions:\n",
    "        center = unit['center']\n",
    "        plt.scatter(center[0], center[1], color='r')\n",
    "\n",
    "        for endpoint in unit['endpoints']:\n",
    "            if endpoint is None:\n",
    "                continue\n",
    "            plt.scatter(endpoint[0], endpoint[1], color='g')\n",
    "            plt.plot([center[0], endpoint[0]], [center[1], endpoint[1]], color='r')\n",
    "\n",
    "        \n",
    "    for unit in grouped_points_real:\n",
    "        center = unit['center']\n",
    "        plt.scatter(center[0], center[1], color='b')\n",
    "\n",
    "        for endpoint in unit['points']:\n",
    "            if endpoint is None:\n",
    "                continue\n",
    "            plt.scatter(endpoint[0], endpoint[1], color='k')\n",
    "            plt.plot([center[0], endpoint[0]], [center[1], endpoint[1]], color='b')\n",
    "\n",
    "\n",
    "    # Formatting the plot\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Position (meters)')\n",
    "    plt.title('Point Tracking Data with Scissor Mechanism')\n",
    "    # set x and y limits \n",
    "    plt.xlim(0, 0.6)\n",
    "    plt.ylim(0, 0.06)\n",
    "    # plt.legend()\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    if save: \n",
    "        plt.savefig(os.path.join(video_dir, f'images/scissor_mechanism_{frame_id}.png'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "96678ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slops = []\n",
    "scissor_positions = scissor.calculate_endpoints()\n",
    "for real_unit, sim_unit in zip(grouped_points_real, scissor_positions):\n",
    "    x_slop = real_unit['center'][0] - sim_unit['center'][0]\n",
    "    y_slop = real_unit['center'][1] - sim_unit['center'][1]\n",
    "    slops.append([x_slop, y_slop])\n",
    "    \n",
    "    for endpoint_real, endpoint_sim in zip(real_unit['points'], sim_unit['endpoints']):\n",
    "        if endpoint_real is None or endpoint_sim is None:\n",
    "            continue\n",
    "        x_slop = endpoint_real[0] - endpoint_sim[0]\n",
    "        y_slop = endpoint_real[1] - endpoint_sim[1]\n",
    "        slops.append([x_slop, y_slop])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "850a4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slops_unadjusted(slops, frame_id, show=False, save=False, video_dir=video_dir):\n",
    "    slops = np.array(slops)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(range(len(slops)), slops[:,0], label='X Slop')\n",
    "    plt.scatter(range(len(slops)), slops[:,1], label='Y Slop')\n",
    "    plt.legend()\n",
    "    plt.xlabel('point id')\n",
    "    plt.ylabel('Slop')\n",
    "    plt.title(\"Slops between real and simulated points\")\n",
    "    plt.xlim(0, 120)\n",
    "    plt.ylim(-0.25, 0.05)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    if save: \n",
    "        plt.savefig(os.path.join(video_dir, f'images/slops_unadjusteed_{frame_id}.png'));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9452460",
   "metadata": {},
   "outputs": [],
   "source": [
    "slops = np.array(slops)\n",
    "n = np.array(slops).shape[0]\n",
    "# Create a lower triangular matrix with 1's\n",
    "lower_triangular_matrix = np.tril(np.ones((n, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e1031c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slops = np.matmul(np.linalg.inv(lower_triangular_matrix),slops[:,0])\n",
    "y_slops = np.matmul(np.linalg.inv(lower_triangular_matrix),slops[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "786b9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slops_adjusted(x_slops, y_slops, frame_id, show=False, save=False,video_dir=video_dir):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(range(len(x_slops)), x_slops, label='X Slop')\n",
    "    plt.scatter(range(len(y_slops)), y_slops, label='Y Slop')\n",
    "    plt.legend()\n",
    "    plt.xlabel('point id')\n",
    "    plt.ylabel('Slop')\n",
    "    plt.title(\"Slops between real and simulated points\")\n",
    "    plt.xlim(0, 120)\n",
    "    plt.ylim(-0.05, 0.05)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    if save: \n",
    "        plt.savefig(os.path.join(video_dir, f'images/slops_adjusted_{frame_id}.png'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_points[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_points_real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a860929",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_points_real[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(video_dir, 'images')):\n",
    "    os.makedirs(os.path.join(video_dir, 'images'))\n",
    "for frame in range(df.shape[0]):\n",
    "    positions = np.asarray([[float(df.iloc[frame,1:][i].replace('(','').replace(')','').split(',')[0]), float(df.iloc[frame,1:][i].replace('(','').replace(')','').split(',')[1])] for i in range(df.shape[1]-1)])\n",
    "    \n",
    "    # get the 3 points with the smallest x values\n",
    "    sorted_positions = positions[positions[:,0].argsort()]\n",
    "    \n",
    "    center_pos = sorted_positions[0, 1]\n",
    "    \n",
    "    center_points = [p for p in sorted_positions if np.linalg.norm(p[1]-center_pos) <= 5e-3]\n",
    "    \n",
    "    grouped_points_real = group_points(sorted_positions, 1.055e-2)\n",
    "    angle = angle_between_points(grouped_points_real[0]['points'][0], grouped_points_real[0]['center'], grouped_points_real[0]['points'][2])\n",
    "    length = np.linalg.norm(sorted_positions[0] - sorted_positions[1])*2\n",
    "    # Define parameters for the scissor mechanism\n",
    "    link_length = length # Length of each scissor link\n",
    "    angle = angle  # Angle between scissor arms\n",
    "    offset_position = sorted_positions[0,:]  # Starting position of the first scissor center\n",
    "    offset_rotation = offset_angle  # No rotation applied to the entire scissor mechanism\n",
    "    num_cells = 10  # Number of scissor units\n",
    "\n",
    "    # Create the scissor mechanism and display positions\n",
    "    scissor = ScissorMechanism(link_length, angle, offset_position, offset_rotation, num_cells)\n",
    "    \n",
    "    scissor_positions = scissor.calculate_endpoints()\n",
    "    slops = []\n",
    "    for real_unit, sim_unit in zip(grouped_points_real, scissor_positions):\n",
    "        x_slop = real_unit['center'][0] - sim_unit['center'][0]\n",
    "        y_slop = real_unit['center'][1] - sim_unit['center'][1]\n",
    "        slops.append([x_slop, y_slop])\n",
    "        \n",
    "        for endpoint_real, endpoint_sim in zip(real_unit['points'], sim_unit['endpoints']):\n",
    "            if endpoint_real is None or endpoint_sim is None:\n",
    "                continue\n",
    "            x_slop = endpoint_real[0] - endpoint_sim[0]\n",
    "            y_slop = endpoint_real[1] - endpoint_sim[1]\n",
    "            slops.append([x_slop, y_slop])\n",
    "\n",
    "    slops = np.array(slops)\n",
    "            \n",
    "    n = slops.shape[0]\n",
    "    # Create a lower triangular matrix with 1's\n",
    "    lower_triangular_matrix = np.tril(np.ones((n, n)))\n",
    "    \n",
    "    x_slops = np.matmul(np.linalg.inv(lower_triangular_matrix),slops[:,0])\n",
    "    y_slops = np.matmul(np.linalg.inv(lower_triangular_matrix),slops[:,1])\n",
    "    print(np.median(abs(x_slops)))\n",
    "    print(np.median(abs(y_slops)))\n",
    "    print(np.mean(abs(x_slops)))\n",
    "    print(np.mean(abs(y_slops)))\n",
    "    \n",
    "    plot_real_v_sim(scissor_positions, grouped_points_real, frame_id=frame, save=True, video_dir=video_dir);\n",
    "    plot_slops_unadjusted(slops, frame_id=frame, save=True, video_dir=video_dir);\n",
    "    plot_slops_adjusted(x_slops, y_slops, frame_id=frame, save=True, video_dir=video_dir);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a47017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "import glob\n",
    "if not os.path.exists(os.path.join(video_dir, 'gifs')):\n",
    "    os.makedirs(os.path.join(video_dir, 'gifs'))\n",
    "for image_type in ['scissor_mechanism', 'slops_unadjusteed', 'slops_adjusted']:\n",
    "    # Set the path pattern for the PNG images\n",
    "    png_files = glob.glob(os.path.join(video_dir, f\"images/{image_type}_*.png\"))\n",
    "\n",
    "    # Custom sorting function to sort by the numeric frame ID\n",
    "    def sort_by_frame_id(file_name):\n",
    "        # Use regular expression to extract the numeric part of the file name\n",
    "        match = re.search(r\"(\\d+).png\", file_name)\n",
    "        return int(match.group(1)) if match else 0\n",
    "\n",
    "    # Sort the files by the numeric frame ID\n",
    "    png_files_sorted = sorted(png_files, key=sort_by_frame_id)\n",
    "\n",
    "    # Load the images\n",
    "    images = [Image.open(png) for png in png_files_sorted]\n",
    "\n",
    "    # Save the images as a GIF\n",
    "    images[0].save(os.path.join(video_dir, f'gifs/{image_type}.gif'), save_all=True, append_images=images[1:], optimize=False, duration=100, loop=0)\n",
    "\n",
    "    print(f\"GIF saved as '{image_type}.gif'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(abs(x_slops))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93461775",
   "metadata": {},
   "source": [
    "## sim2sim version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c3af07c9b62e9e5a46520c50c603ac08cca123627f098bd5ae6d6af0e7d15f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
